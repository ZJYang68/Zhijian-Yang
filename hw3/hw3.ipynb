{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyAsw2JuVfaM"
      },
      "source": [
        "# Homework 3: Coding\n",
        "\n",
        "**Due Monday October 5th, 11:59pm.**\n",
        "\n",
        "**Submit hw3.ipynb file to Gradescope (you may submit as many times as you'd like before the deadline).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYCgbkJwVg7r",
        "outputId": "272a3755-3baa-482a-9394-f57befeedecf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "Import libraries that you might require.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "np.__version__\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.16.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccn-TAICxI50"
      },
      "source": [
        "\"\"\"\n",
        "FOR COLAB USERS ONLY\n",
        "\n",
        "Run the following code to upload and unzip the data into the Colab environment.\n",
        "\n",
        "\"\"\"\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# ! unzip hw3_q3.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSmmDxXGV4y5"
      },
      "source": [
        "# Question 3: Kernel Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7yZ037YC4QL"
      },
      "source": [
        "In this question, you are going to implement a Kernel Regression model using Gaussian kernel method.\n",
        "\n",
        "Given a training dataset $S=((\\mathbf{x_1}, y_1), \\ldots , (\\mathbf{x_n}, y_n))$, and kernel function $K(\\cdot,\\cdot)$, the predicted value $\\hat{y}$ of an input data $\\mathbf{x}$ is:\n",
        "$$\\hat{y}(\\mathbf{x}) = \\frac{\\sum_{i=1}^n K(\\mathbf{x}, \\mathbf{x_i}) y_i}{\\sum_{i=1}^n K(\\mathbf{x}, \\mathbf{x_i}) }$$, where $K(\\mathbf{x}, \\mathbf{x_i}) = \\text{exp}(\\frac{-||\\mathbf{x} - \\mathbf{x_i}||_2^2}{\\sigma^2})$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pmc4JtiRDScP"
      },
      "source": [
        "## Question 3.1 Build the model.\n",
        "Fill in your code for function **GaussianKernel** and **kernelRegression**. For $\\sigma = \\{0.01, 0.05, 0.1, 0.15, 0.2, 0.5, 1.0\\}$, fill in your code for function **evaluation** to compute the means squared error of the model for each $\\sigma$ value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzgsvCGsVvmt"
      },
      "source": [
        "\"\"\"\n",
        "Reads the data.\n",
        "\"\"\"\n",
        "# TODO your code here\n",
        "\n",
        "X_test = \n",
        "y_test = \n",
        "X_train = \n",
        "y_train = \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTAjfAVM7Lc7"
      },
      "source": [
        "# build the model\n",
        "def GaussianKernel(sigma, vec1, vec2):\n",
        "    \"\"\"\n",
        "    Computes the gaussian kernel between two d-dim vectors\n",
        "    Args:\n",
        "        sigma: a single floating number\n",
        "        vec1: (d,)-shape numpy vector\n",
        "        vec2: (d,)-shape numpy vector\n",
        "    Returns:\n",
        "        distance: a single floating number\n",
        "    \"\"\"\n",
        "    # TODO your code here: Question 3.1\n",
        "    distance = \n",
        "    return distance\n",
        "\n",
        "\n",
        "def kernelRegression(X_train, y_train, X_test, sigma):\n",
        "    \"\"\"\n",
        "    Computes the predicted values for test set X_test based on kernel regression model\n",
        "    Args:\n",
        "        X_train: (n,p) feature matrix of training set\n",
        "        y_train: truth value of training set\n",
        "        X_test: feature matrix of test set\n",
        "        sigma: hyperparameter for Gaussian kernel\n",
        "    Returns:\n",
        "        y_predict: list of predicted target values for X_test\n",
        "    \"\"\"\n",
        "    # TODO your code here: Question 3.1\n",
        "    # You need to call the function \"GaussianKernel\" here\n",
        "    y_predict = [0]*len(X_test)    # initialzation\n",
        "\n",
        "    return y_predict\n",
        "\n",
        "\n",
        "# evaluate the model\n",
        "def evaluation(y_predict, y_true):\n",
        "    \"\"\"\n",
        "    Computes the mean squared error for regression task.\n",
        "    \n",
        "    Args:\n",
        "        y_predict: list of predicted target values\n",
        "        y_true: list or numpy array of true target values\n",
        "    \n",
        "    Returns:\n",
        "        error: a floating point number representing the error for a validation or test set\n",
        "    \"\"\"    \n",
        "    # TODO your code here: Question 3.1\n",
        "    # you can use the sklearn libary mean_squared_error\n",
        "    \n",
        "    error = \n",
        "    return error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKhXx-ObVicq"
      },
      "source": [
        "## Question 3.2 Analysis of the model.\n",
        "Similar to what you did for HW2, plot a figure for each sigma value of predicted regression line and scatter plot of data points in the test set. Report the sigma value with the smallest MSE for the test set. How the value of sigma affects the kernel regression model? Compare the figures in this question with the figure in question 1.1 of HW2. Describe your findings based on the comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti8Ys2ix_1Ik"
      },
      "source": [
        "# plot the kernel regression result\n",
        "def plotRegression(X_train, y_train, X_test, y_test, y_pred, sigma):\n",
        "    \"\"\"\n",
        "    Plot the predicted regression line and scattor plot of data points in the test set\n",
        "    Args:\n",
        "        sigma: sigma value for Gqussian kernel\n",
        "    Return:\n",
        "        error: mean squared error of the model given the sigma\n",
        "    \"\"\"\n",
        "    error = evaluation(y_pred, y_test)\n",
        "    x = np.linspace(0,1,1000)\n",
        "    y_regression = kernelRegression(X_train, y_train, x, sigma)\n",
        "    \n",
        "    plt.plot(x, y_regression, '-', color = 'red')\n",
        "    plt.scatter(X_test, y_test, alpha = 0.75)\n",
        "    plt.title('Gaussian kernel regression with sigma = ' + str(sigma) + '\\n MSE = ' + str(error))\n",
        "    plt.xlabel('feature value')\n",
        "    plt.ylabel('target value')\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "def main(X_train, y_train, X_test, y_test, sigma_set):\n",
        "    \"\"\"\n",
        "    Build the Gaussian kernel regression for each sigma in sigma_set, and then plot the result\n",
        "    \"\"\"\n",
        "\n",
        "    for sigma in sigma_set:\n",
        "        y_pred = kernelRegression(X_train, y_train, X_test, sigma)\n",
        "        plotRegression(X_train, y_train, X_test, y_test, y_pred, sigma)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJVveIQ57fH2"
      },
      "source": [
        "\"\"\" \n",
        "Finally, we call the main function.\n",
        "\"\"\"\n",
        "sigma_set = [0.01, 0.05, 0.1, 0.15, 0.2, 0.5, 1.0]\n",
        "main(X_train, y_train, X_test, y_test, sigma_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfld3QqlkFdf"
      },
      "source": [
        "The answer for quesiton 3.2 should be written on the PDF file that you submit to Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ6t84WwXrK6"
      },
      "source": [
        "# Question 4: Logistic Regression and Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56bWi7_PFbAd"
      },
      "source": [
        "\"\"\"\n",
        "Import required libraries.\n",
        "\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIDoP-L3IL4V"
      },
      "source": [
        "\"\"\"\n",
        "FOR COLAB USERS ONLY\n",
        "\n",
        "Run the following code to upload and unzip the data into the Colab environment.\n",
        "\n",
        "\"\"\"\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "! unzip hw3_house_sales.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWme3egnIQvP"
      },
      "source": [
        "Implement the following functions for question 1. Please use the sklearn implementation of linear regression or other imports beyond those listed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoPLNR8xIY3b"
      },
      "source": [
        "\"\"\"\n",
        "load data, a const dimension (for weight b) is already included in X.\n",
        "\"\"\"\n",
        "X_train = pd.read_csv('X_train.csv')\n",
        "X_test = pd.read_csv('X_test.csv')\n",
        "y_train = pd.read_csv('y_train.csv')\n",
        "y_test = pd.read_csv('y_test.csv')\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "print(X_test.head(5))\n",
        "print(y_test.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHtOjixmSh86"
      },
      "source": [
        "\"\"\"\n",
        "Do some data preparation, convert dataframe to numpy array\n",
        "\"\"\"\n",
        "n_features = X_train.shape[1]\n",
        "\n",
        "w = np.zeros((1, n_features))\n",
        "\n",
        "# turn dataframe to np array\n",
        "X_train = X_train.values\n",
        "X_test = X_test.values\n",
        "\n",
        "y_train = y_train.values\n",
        "y_test = y_test.values\n",
        "\n",
        "m_train =  X_train.shape[0]\n",
        "m_test =  X_test.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVmVoQfAUkqe"
      },
      "source": [
        "**Logistic regression with scikit** Fill in the logisticRegressionScikit() function. Report the weights, training accuracy, and the test accuracy. We will not use any penalty here, so set the parameters penalty = 'none', solver = 'saga'.Also, we will use 2000 iterations for a fair comparison to later algorithms, so also set the parameter max_iter=2000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psZtnphuGJSj"
      },
      "source": [
        "def LogisticRegressionScikit(X_train, y_train, X_test, y_test):\n",
        "\n",
        "    \"\"\"\n",
        "    Computes logistic regression with scikit-learn.\n",
        "    \n",
        "    Args:\n",
        "        X_train: feature matrix of training set, np array of (n, p)\n",
        "                 where n is the number of training observations, \n",
        "                 p is the number of features\n",
        "        y_train: truth value of training set, np array of (n, 1)\n",
        "\n",
        "        X_test: feature matrix of test set, np array of (m, p)\n",
        "                 where m is the number of test observations,\n",
        "                 p is the number of features\n",
        "        y_test: truth value of test set, np array of (m, 1)\n",
        "\n",
        "    Returns:  \n",
        "        w: numpy array of learned coefficients\n",
        "        y_pred: numpy array of predicted labels for the test data\n",
        "        score: accuracy of test data\n",
        "    \"\"\"\n",
        "\n",
        "    return coef, y_pred, score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GJ7cAepGwqp"
      },
      "source": [
        "\"\"\"\n",
        "Results for logistic regression Scikit function\n",
        "\"\"\"\n",
        "\n",
        "coef_scikit, y_pred_scikit, acc_scikit = LogisticRegressionScikit(X_train, y_train, X_test, y_test)\n",
        "\n",
        "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(acc_scikit))\n",
        "print('logistic regression coefficient:', coef_scikit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I06kejCVxv8"
      },
      "source": [
        "**Logistic regression with simple gradient descent** Fill in the LogisticRegressionGD() function. To do that, two helper functions sigmoid_activation(), to calculate the sigmoid function result, and model_optimize(), to calculate the gradient of w, will be needed. Both helper functions can be used in the following AdaGrad optimization function. Use a learning rate of $10^{−4}$, run with 2000 iterations. Report the weights and accuracy. Keep track of the accuracy every 100 iterations in the training set. It will be used later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpkEe4mkQ19p"
      },
      "source": [
        "def sigmoid_activation(result):\n",
        "    \"\"\"\n",
        "    Calculates the sigmoid function.\n",
        "    \n",
        "    Args:\n",
        "        x: numpy array of input, of shape (1, n)\n",
        "           where n is the number of observations\n",
        "        \n",
        "    Returns:\n",
        "        final_result: numpy array of sigmoid result, of shape (1, n)\n",
        "                      where n is the number of observations\n",
        "    \"\"\"\n",
        "\n",
        "    return final_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TScnzScyaCZM"
      },
      "source": [
        "We add a predict() function here to threshold probability prediction into binary classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypERS0E1SLul"
      },
      "source": [
        "def predict(final_pred, n):\n",
        "    \"\"\"\n",
        "    Predict labels from probability to 0/1 label, threshold 0.5.\n",
        "    \n",
        "    Args:\n",
        "        final_pred: numpy array of probabilty that each sample belonging to class 1, of shape (1, n)\n",
        "                    where n is the number of observations \n",
        "        \n",
        "    Returns:\n",
        "        y_pred: numpy array of label of each sample, of shape (1, n)\n",
        "                where n is the number of observations\n",
        "    \"\"\"\n",
        "    \n",
        "    return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BJvPP71bFs5"
      },
      "source": [
        "**Remember to derive the gradient, write down the weight update formula, and hand in them to the latex submission!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RGSw2AyQ_kE"
      },
      "source": [
        "def model_optimize(w, X, Y):\n",
        "    \n",
        "    \"\"\"\n",
        "    Calculates gradient of the weights.\n",
        "    \n",
        "    Args:\n",
        "        X: numpy array of training samples of shape (n, p)\n",
        "           where n is the number of observations\n",
        "           p is the number of features\n",
        "        Y: numpy array of training labels of shape (n, 1)\n",
        "        w: numpy array of weights of shape (1, p)\n",
        "    Returns:\n",
        "        dw: the gradient of the weights of shape (1, p)\n",
        "    \"\"\"\n",
        "\n",
        "    return dw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI5Fdz6ZRUQ9"
      },
      "source": [
        "def LogisticRegressionGD(w, X, Y, learning_rate, num_iterations):\n",
        "    \"\"\"\n",
        "    Uses batch gradient descent to update weights for logistic regression.\n",
        "\n",
        "    Args:       \n",
        "        w: numpy array of initial weights of shape (1, p)\n",
        "           where p is the number of features\n",
        "        X: numpy array of training samples of shape (n, p)\n",
        "           where n is the number of observations\n",
        "        Y: numpy array of training labels of shape (n, 1)\n",
        "        learning_rate: float number learning rate to update w\n",
        "        num_iterations: int number of iterations to update w\n",
        "    \n",
        "    Returns:  \n",
        "        coeff: numpy array of weights after optimization of shape (1, p)\n",
        "        accuracies: a list of accuracy at each hundred's iteration. With 2000 iterations, \n",
        "                    accuracies should be a list of size 21 (starting from 0)\n",
        "    \"\"\"\n",
        "    \n",
        "    return coeff, accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ2M5WETZ4f0"
      },
      "source": [
        "**Logistic regression with AdaGrad** Fill in the LogisticRegressionAda() function. Use a learning rate of $10^{−4}$, run with 2000 iterations. Report the weights and accuracy. Keep track of the accuracy every 100 iterations in the training set. It will be used later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIO8iOzJRscr"
      },
      "source": [
        "def LogisticRegressionAda(w, X, Y, learning_rate, num_iterations):\n",
        "\n",
        "    \"\"\"\n",
        "    Use AdaGrad to update weights.\n",
        "    \n",
        "    Args:       \n",
        "        w: numpy array of initial weights of shape (1, p)\n",
        "           where p is the number of features\n",
        "        X: numpy array of training samples of shape (n, p)\n",
        "           where n is the number of observations\n",
        "        Y: numpy array of training labels of shape (n, 1)\n",
        "        learning_rate: float number learning rate to update w\n",
        "        num_iterations: int number of iterations to update w\n",
        "    \n",
        "    Returns:  \n",
        "        coeff: numpy array of weights after optimization of shape (1, p)\n",
        "        accuracies: a list of accuracy at each hundred's iteration. With 2000 iterations, \n",
        "                    accuracies should be a list of size 21 (starting from 0)\n",
        "    \"\"\"\n",
        "\n",
        "    accuracies = []\n",
        "    \n",
        "    return coeff, accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLQA1SHAaRQn"
      },
      "source": [
        "Now we start to use our dataset and construct model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "845srOn1DG7j"
      },
      "source": [
        "Model construction for GD logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpa3o6VMTdTc"
      },
      "source": [
        "\"\"\"\n",
        "Results for gradient descent weight update\n",
        "\"\"\"\n",
        "\n",
        "# Gradient Descent\n",
        "coeff_GD, acc_GD = LogisticRegressionGD(w, X_train, y_train, learning_rate=0.0001,num_iterations=2000)\n",
        "\n",
        "# predict probability\n",
        "final_train_pred_GD = sigmoid_activation(np.dot(coeff_GD, X_train.T) )\n",
        "final_test_pred_GD = sigmoid_activation(np.dot(coeff_GD, X_test.T) )\n",
        "# predict label\n",
        "y_train_pred_GD = predict(final_train_pred_GD, m_train)\n",
        "y_test_pred_GD = predict(final_test_pred_GD, m_test)\n",
        "\n",
        "print('Optimized weights for GD', coeff_GD)\n",
        "\n",
        "print('Training Accuracy for GD', accuracy_score(y_train_pred_GD.T, y_train))\n",
        "print('Test Accuracy for GD', accuracy_score(y_test_pred_GD.T, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cSHZdx-amhq"
      },
      "source": [
        "Model construction for AdaGrad logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idXxXj3zTcwp"
      },
      "source": [
        "\"\"\"\n",
        "Results for AdaGrad Descent weight update\n",
        "Please comment out these statements before converting to .py file and submitting.\n",
        "\"\"\"\n",
        "coeff_Ada, acc_Ada = LogisticRegressionAda(w, X_train, y_train, learning_rate=0.0001, num_iterations=2000)\n",
        "\n",
        "# predict probability\n",
        "final_train_pred_Ada = sigmoid_activation(np.dot(coeff_Ada, X_train.T) )\n",
        "final_test_pred_Ada = sigmoid_activation(np.dot(coeff_Ada, X_test.T) )\n",
        "\n",
        "# predict label\n",
        "y_train_pred_Ada = predict(final_train_pred_Ada, m_train)\n",
        "y_test_pred_Ada = predict(final_test_pred_Ada, m_test)\n",
        "\n",
        "print('Optimized weights for Ada', coeff_Ada)\n",
        "\n",
        "print('Training Accuracy for Ada', accuracy_score(y_train_pred_Ada.T, y_train))\n",
        "print('Test Accuracy for Ada', accuracy_score(y_test_pred_Ada.T, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egjfJWHWasSQ"
      },
      "source": [
        "Plot accuracy vs iteration for GD and AdaGrad. Compare the performance difference. Briefly explain the reason."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFiMS7M4UEU7"
      },
      "source": [
        "\"\"\"\n",
        "Plot accuracy vs iteration for GD and AdaGrad\n",
        "\"\"\"\n",
        "plt.plot(acc_GD, label='GD')\n",
        "plt.plot(acc_Ada, label='AdaGrad')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title('Accuracy improvement over time')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}