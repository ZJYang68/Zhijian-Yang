{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAqXcDzKwj4D"
      },
      "source": [
        "# Homework 4: Coding\n",
        "\n",
        "**Due Monday October 7th, 11:59pm.**\n",
        "\n",
        "**This is an individual assignment.**\n",
        "\n",
        "**In order to avoid module version issues, please complete this assignment on Colab.**\n",
        "\n",
        "**Submit hw3.py file to Gradescope (note there is no autograder for this assignment).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqy1LtZCIQuG"
      },
      "source": [
        "\"\"\"\n",
        "Import libraries that you might require\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import operator\n",
        "from sklearn.metrics import accuracy_score\n",
        "import sklearn.model_selection as ms\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qmuIim2IQuK"
      },
      "source": [
        "\"\"\"\n",
        "Load data (MNIST digits dataset).\n",
        "\n",
        "Note that we will skip the validation phase for\n",
        "this exercise as by now you are pretty familiar with the typical Machine Learning\n",
        "pipeline.\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "print(digits.data.shape)\n",
        "\n",
        "X = digits['data']\n",
        "y = digits['target']\n",
        "\n",
        "np.random.seed(100)\n",
        "p = np.random.permutation(len(X))\n",
        "X, y = X[p], y[p]\n",
        "\n",
        "X_train, y_train = X[:1500], y[:1500]\n",
        "X_test, y_test = X[1500:], y[1500:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmURULzoXCvq"
      },
      "source": [
        "# Question 2: Performance Comparisons for three ML algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BulL-miy2W1a"
      },
      "source": [
        "## 2.0 Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En_czqit2Wip"
      },
      "source": [
        "def train(models, X_train, y_train, X_test, y_test):\n",
        "  \"\"\"\n",
        "  Trains several models and returns the test accuracy for each of them\n",
        "  Args:\n",
        "      models: list of model objects\n",
        "  Returns:\n",
        "      score (float): list of accuracies of the different fitted models on test set\n",
        "  \"\"\"\n",
        "\n",
        "  # To complete: train and test each model in a for lop\n",
        "\n",
        "  accuracies = []\n",
        "\n",
        "\n",
        "  return accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_EG5SVkxrzs"
      },
      "source": [
        "## 2.1 Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KC2aNmoXCvr"
      },
      "source": [
        "def modelRF(n_estimators):\n",
        "  \"\"\"\n",
        "  Creates model objects for the Random Forest Classifier.\n",
        "  See the documentation in sklearn here:\n",
        "  https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "  Args:\n",
        "    n_estimators: list of hyper parameters\n",
        "  return:\n",
        "    list of classifiers\n",
        "  \"\"\"\n",
        "  \n",
        "  list_n_estimators = n_estimators\n",
        "  random_state = 20 # Do not change this random_state\n",
        "\n",
        "\n",
        "  objs_RFC = []\n",
        "  \n",
        "  # To complete: Create a list of objects for the classifier for each of the above \"n_estimators\"\n",
        "\n",
        "  return objs_RFC\n",
        " \n",
        "\n",
        "# To complete: call the above function to train and test the Random Forest Classifier\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJrNWsSgz6yh"
      },
      "source": [
        "## 2.2 Kernel SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLg9s5T4pAus"
      },
      "source": [
        "def modelKSVM():\n",
        "  \"\"\"\n",
        "  Creates model objects for the Kernel SVM.\n",
        "  See the documentation in sklearn here:\n",
        "  https://scikit-learn.org/stable/modules/svm.html\n",
        "  \"\"\"\n",
        "  \n",
        "  list_kernel_type = ['linear', 'poly', 'rbf']\n",
        "  random_state = 20 # Do not change this random_state\n",
        "\n",
        "  objs_KSVM = []\n",
        "  \n",
        "  # To complete: Create a list of objects for the classifier for each of the above \"kernel\"\n",
        "\n",
        "  return objs_KSVM\n",
        "\n",
        "# To complete: Call the above function to train and test the Random Forest Classifier\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vSc15PpxyS2"
      },
      "source": [
        "## 2.3 Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejmbeisHpBdO"
      },
      "source": [
        "def modelMLP():\n",
        "  \"\"\"\n",
        "  Creates model objects for the Multi Layered Perceptron.\n",
        "  See the documentation in sklearn here:\n",
        "  https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "  \"\"\"\n",
        "  \n",
        "  \n",
        "  layerSizes = [(3), (10), (10,10,10), (20,50,20)]\n",
        "  random_state = 20 # Do not change this random_state\n",
        "  max_iter = 2000 # fixed max_iter\n",
        "  \n",
        "  objs_MLP = []\n",
        "\n",
        "  # To complete: Create a list of objects for the classifier for each of the above \"n_estimators\"\n",
        "\n",
        "  return objs_MLP\n",
        "\n",
        "# To complete: Call the above function to train and test the Multi Layer Perceptron\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82t0UEM2fYOM"
      },
      "source": [
        "## 2.4 AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mceSBJ_fXU0"
      },
      "source": [
        "def modelAdaBoost():\n",
        "  \"\"\"\n",
        "  Creates model objects for the AdaBoost.\n",
        "  See the documentation in sklearn here:\n",
        "  https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
        "  \"\"\"\n",
        "  num_estimators = [1,5,10,50,100,150]\n",
        "  learning_rate = 0.1\n",
        "  max_depth = 3\n",
        "  random_state = 20 # Do not change this random_state\n",
        "  # To complete: Create a list of objects for the classifier for each of combination of above n_estimators and learning_rate\n",
        "  obj_boost = []\n",
        "\n",
        "  return obj_boost\n",
        "\n",
        "# To complete: Call the above function to train and test the AdaBoost Classifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9fIU8eP_X1u"
      },
      "source": [
        "# Question 3.2 Convolutional Neural Networks  \n",
        "In this assignment you will be training a Convolutional Neural Network on  \n",
        "the Fashion MNIST dataset.  \n",
        "\n",
        "You may find more information about the dataset [here](https://github.com/zalandoresearch/fashion-mnist).  \n",
        "For this assignment we have already loaded the dataset for you.  \n",
        "  \n",
        "You will be using PyTorch for implementing your CNN. \n",
        "\n",
        "**We highly recommend following [this tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) for this question** as well as referring to the [official documentation](https://pytorch.org/docs/stable/nn.html) if you are unfamiliar with Pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL1q9V0O13B6"
      },
      "source": [
        "## Setup: Load Tensorboard\n",
        "\n",
        "The below code is used to load [Tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard), which is used to visualize the training and execution of your neural network.\n",
        "\n",
        "Run the below cells and click on the Tensorboard link produced by the third cell below while your network is training (Section 3.2.5) to plot the accuracy and loss curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWS884Qt13Bd"
      },
      "source": [
        "!rm -r -f ./logs\n",
        "\n",
        "LOG_DIR = './logs'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "!if [ -f ngrok ] ; then echo \"Ngrok already installed\" ; else wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1 && unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1 ; fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvbnO1JX13BO"
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0lafkzw13Ak"
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print('Tensorboard Link: ' +str(json.load(sys.stdin)['tunnels'][0]['public_url']))\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSeELIRYjGSG"
      },
      "source": [
        "## Setup: Logger\n",
        "\n",
        "Please look at the functions the logger class provides. You may use them to log   \n",
        "training metrics like loss, accuracy and even some selected images and their  \n",
        "labels to see how network parameters change during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipIykcBHt1dk"
      },
      "source": [
        "# Code referenced from https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.misc \n",
        "try:\n",
        "    from StringIO import StringIO  # Python 2.7\n",
        "except ImportError:\n",
        "    from io import BytesIO         # Python 3.x\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    \n",
        "    def __init__(self, log_dir):\n",
        "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
        "        self.writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "    def scalar_summary(self, tag, value, step):\n",
        "        \"\"\"Log a scalar variable.\"\"\"\n",
        "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
        "        self.writer.add_summary(summary, step)\n",
        "\n",
        "    def image_summary(self, tag, images, step):\n",
        "        \"\"\"Log a list of images.\"\"\"\n",
        "\n",
        "        img_summaries = []\n",
        "        for i, img in enumerate(images):\n",
        "            # Write the image to a string\n",
        "            try:\n",
        "                s = StringIO()\n",
        "            except:\n",
        "                s = BytesIO()\n",
        "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
        "\n",
        "            # Create an Image object\n",
        "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
        "                                       height=img.shape[0],\n",
        "                                       width=img.shape[1])\n",
        "            # Create a Summary value\n",
        "            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n",
        "\n",
        "        # Create and write Summary\n",
        "        summary = tf.Summary(value=img_summaries)\n",
        "        self.writer.add_summary(summary, step)\n",
        "        \n",
        "    def histo_summary(self, tag, values, step, bins=1000):\n",
        "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
        "\n",
        "        # Create a histogram using numpy\n",
        "        counts, bin_edges = np.histogram(values, bins=bins)\n",
        "\n",
        "        # Fill the fields of the histogram proto\n",
        "        hist = tf.HistogramProto()\n",
        "        hist.min = float(np.min(values))\n",
        "        hist.max = float(np.max(values))\n",
        "        hist.num = int(np.prod(values.shape))\n",
        "        hist.sum = float(np.sum(values))\n",
        "        hist.sum_squares = float(np.sum(values**2))\n",
        "\n",
        "        # Drop the start of the first bin\n",
        "        bin_edges = bin_edges[1:]\n",
        "\n",
        "        # Add bin edges and counts\n",
        "        for edge in bin_edges:\n",
        "            hist.bucket_limit.append(edge)\n",
        "        for c in counts:\n",
        "            hist.bucket.append(c)\n",
        "\n",
        "        # Create and write Summary\n",
        "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
        "        self.writer.add_summary(summary, step)\n",
        "        self.writer.flush()\n",
        "logger = Logger('./logs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIkuagd9_X1v"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvDAZByo_X1w"
      },
      "source": [
        "##3.2.1 Loading the Dataset\n",
        "The output of torchvision datasets are PILImage images of range [0, 1].  \n",
        "We transform them to Tensors of normalized range [-1, 1].  \n",
        "```Transforms.Normalize((mean,),(std,))``` basically manipulates the values of a pixel such that  \n",
        "$$New\\_Value = \\frac{Old\\_Value - Mean}{Std}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK7aviwP_X1x"
      },
      "source": [
        "# Define a transform to normalize the data\n",
        "\n",
        "#TODO : Set the value of mean and the standard deviation to \n",
        "#       normalize the image from range [0,1] to the range [-1, 1]\n",
        "\n",
        "\n",
        "#Begin Your Code\n",
        "\n",
        "mean = \n",
        "std = \n",
        "\n",
        "#End Your Code\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((mean,), (std,))\n",
        "                                ])\n",
        "\n",
        "\n",
        "#TODO : Select suitable value of batch_sizes.\n",
        "\n",
        "#Begin Your Code\n",
        "\n",
        "train_batch_size = \n",
        "test_batch_size = \n",
        "\n",
        "#End Your Code\n",
        "\n",
        "# Download and load the training data\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=train_batch_size, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "# Classes\n",
        "classes = {       0 :'T-shirt/top',\n",
        "                  1 :'Trouser',\n",
        "                  2 :'Pullover',\n",
        "                  3 :'Dress',\n",
        "                  4 :'Coat',\n",
        "                  5 :'Sandal',\n",
        "                  6 :'Shirt',\n",
        "                  7 :'Sneaker',\n",
        "                  8 :'Bag',\n",
        "                  9 :'Ankle boot'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFBh6ZhO_X1y"
      },
      "source": [
        "##3.2.2 The Dataset\n",
        "Here we show some images of the dataset.  \n",
        "See how many of the categories can you recognise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX5Irrw-_X1z"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import numpy as np\n",
        "\n",
        "# Functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    \n",
        "    figure(num=None, figsize=(8, 6), dpi=150, edgecolor='k')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le9N91sO_X11"
      },
      "source": [
        "##3.2.3 Create your Convolutional Neural Network\n",
        "Create the CNN with layers and hyperparameter sets as mentioned in the LaTeX pdf for full credit.  \n",
        "You are, however, free to change the architecture as long as you achieve accuracy better than the architecture shown in the LaTeX pdf.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWmqB-1b_X11"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        #TODO : Design your network, you are allowed to explore your own architecture\n",
        "        #       But you should achieve a better overall accuracy than the baseline network.\n",
        "        #       Also, if you do design your own network, include an explanation \n",
        "        #       for your choice of network and how it may be better than the \n",
        "        #       baseline network in your latex.\n",
        "        \n",
        "        #Begin Your Code\n",
        "\n",
        "\n",
        "        #End Your Code\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      #TODO : Implement the forward function that applies the layers you have created to the input\n",
        "\n",
        "      #Begin Your Code\n",
        "\n",
        "\n",
        "      #End Your Code\n",
        "\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jieF6xy__X13"
      },
      "source": [
        "##3.2.4 Define a Loss function and optimizer\n",
        "We will be using [Cross Entropy Loss](https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss) and [Adam optimizer](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam).  \n",
        "Note: PyTorch's CrossEntropyLoss combines log softmax and negative log likelihood loss in one class. Make sure you are not computing softmax twice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5-Qn1ef_X13"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "#TODO : Use appropriate loss criterion and optimizer \n",
        "\n",
        "#Begin Your Code\n",
        "\n",
        "criterion = \n",
        "optimizer = \n",
        "\n",
        "#End Your Code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4za3t0b_X15"
      },
      "source": [
        "##3.2.5. Train the network\n",
        "\n",
        "Here we are going to train the network while logging the per batch metrics.  \n",
        "This would take some time to run (5-10 minutes).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7y1qZI1wyvv"
      },
      "source": [
        "overall_step = 0\n",
        "\n",
        "#TODO : Select appropriate number of epochs\n",
        "\n",
        "#Begin Your Code\n",
        "\n",
        "epochs =\n",
        "\n",
        "#End Your Code\n",
        "\n",
        "\n",
        "for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "    running_loss = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        #TODO : Make predictions, calculate accuracy and update your weights once\n",
        "\n",
        "        #Begin Your Code\n",
        "\n",
        "        # zero the parameter gradients\n",
        "\n",
        "        # forward + backward + optimize\n",
        "\n",
        "        #End Your Code\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 200 mini-batches\n",
        "            print('Epoch: %d, Batch: %5d, loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "            #Any thing that is added to the \"info\" gets plotted in tensorboard\n",
        "            #TODO : Add the plots in Tensorboard to the report and explain what is happening\n",
        "            info = { ('loss') : loss.item(),('accuracy'): accuracy.item()}\n",
        "            for tag, value in info.items():\n",
        "                logger.scalar_summary(tag, value, overall_step+1)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RREjngOC_X2F"
      },
      "source": [
        "##3.2.6 Test Accuracy\n",
        "Let us look at how the network performs on the test dataset.  \n",
        "Report your accuracy in your report.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTyWZj2R_X2F"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "#TODO : Report this accuracy in your report.\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCCVNEjV_X2G"
      },
      "source": [
        "##3.2.7 Per Class accuracy\n",
        "Now we see the test accuracy for each class in the test dataset.  \n",
        "Report these accuracies in your report. Also identify the problematic classes.  \n",
        "Can you explain why these classes have significantly lower accuracies compared to other classes? Record your responses in your LaTeX file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO9h7Zbd_X2H"
      },
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnvvXuO2z-kt"
      },
      "source": [
        "\n",
        "# Turning it in\n",
        "\n",
        "**This notebook will not be autograded, so no need to comment out code outside of functions.**\n",
        "\n",
        "1. Download this notebook as a `hw4.py` file with the functions implemented and the sandbox code commented out\n",
        "  - go to \"File -> Download .py\"\n",
        "  \n",
        "2. Submit `hw4.py` file to Gradescope (you can do this as many times as you'd like before the deadline)"
      ]
    }
  ]
}