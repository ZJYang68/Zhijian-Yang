{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.1+"
    },
    "colab": {
      "name": "hw2_2020_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esB_IsP6tXlJ",
        "colab_type": "text"
      },
      "source": [
        "# Homework 2: Coding\n",
        "\n",
        "**Due Monday September 28th, 11:59pm.**\n",
        "\n",
        "**This assignment can be done individually or in groups of two**\n",
        "\n",
        "**Submit hw2.ipynb to Gradescope (One per group and don't forget to add your partner's name)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7A8_5bKoB06",
        "colab_type": "text"
      },
      "source": [
        "### Imports and Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NexR8Vb2oB0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Import required libraries.\n",
        "\n",
        "DON'T comment out these imports when submitting your final hw2.py file.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zjWYTWxb1N0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "FOR COLAB USERS ONLY\n",
        "\n",
        "Run the following code to upload and unzip the data into the Colab environment.\n",
        "\n",
        "Please comment out *everything* in this cell (including the import) when submitting your .py file.\n",
        "\"\"\"\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "! unzip hw2_data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2weGbLJ2txVd",
        "colab_type": "text"
      },
      "source": [
        "## Q1: Least Squares Regression\n",
        "\n",
        "Implement the following functions for question 1. Please **do not** use the sklearn implementation of linear regression or other imports beyond those listed above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4LiXAOQoB1J",
        "colab_type": "text"
      },
      "source": [
        "### L1 and L2 error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0v0785ioB1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L2_error(y, y_hat):\n",
        "    \"\"\"\n",
        "    L2 error loss\n",
        "    \n",
        "    Args:\n",
        "    y ((n,1) np.array): actual labels\n",
        "    y_hat ((n,1) np.array): estimated labels\n",
        "    \n",
        "    Returns:\n",
        "        float: L2 error\n",
        "    \"\"\"\n",
        "    L2_error = 0\n",
        "    return L2_error\n",
        "\n",
        "\n",
        "\n",
        "def L1_error(y, y_hat):\n",
        "    \"\"\"\n",
        "    L1 error loss\n",
        "    \n",
        "    Args:\n",
        "    y ((n,1) np.array): actual labels\n",
        "    y_hat ((n,1) np.array): estimated labels\n",
        "    \n",
        "    Returns:\n",
        "        float: L1 error\n",
        "    \"\"\"\n",
        "    L1_error = 0\n",
        "    \n",
        "    return L1_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asQPLD_HoB1T",
        "colab_type": "text"
      },
      "source": [
        "### Least Square Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVMYIbQGoB1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LinearRegression(train_data, train_labels):\n",
        "    \"\"\"\n",
        "    Runs OLS on the given data.\n",
        "    \n",
        "    Args:\n",
        "        train_data ((n,p) np.array): n is the number of training points and p the number of features\n",
        "        train_labels ((n,1) np.array): training labels for the training data    \n",
        "    \n",
        "    Returns\n",
        "        tuple: (w, b) where w is a (p,1) weight vector, and b the bias term     \n",
        "    \"\"\"\n",
        "    w = np.array()\n",
        "    b = 0\n",
        "    \n",
        "    \n",
        "    return (w,b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-MIQRMFoB1j",
        "colab_type": "text"
      },
      "source": [
        "### Ridge Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8twOpQynoB1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LinearRegressionL2(train_data, train_labels, lambda_):\n",
        "    \"\"\"\n",
        "    Runs linear regression with L2 regularization (ridge) on the given data.\n",
        "    \n",
        "    Args:\n",
        "        train_data ((n,p) np.array): n is the number of training points and p the number of features\n",
        "        train_labels ((n,1) np.array): training labels for the training data    \n",
        "        lambda_  (float): scalar weighting the L2 penalty\n",
        "\n",
        "    Returns\n",
        "        tuple: (w, b) where w is a (p,1) weight vector, and b the bias term  \n",
        "    \"\"\"\n",
        "    w = np.array()\n",
        "    b = 0\n",
        "    \n",
        "    \n",
        "    return (w,b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOv6gZT-oB10",
        "colab_type": "text"
      },
      "source": [
        "### Q1.1.1: Learning Curve\n",
        "\n",
        "Use your implementation of (unregularized) least squares regression to learn a regression model from 10$\\%$ of the training data, then 20$\\%$ of the training data, then 30$\\%$ and so on up to 100$\\%$ (separate files containing r$\\%$ of the training examples are provided under the folder for this problem with file names **Data-set-1/Train-subsets/X$\\_$train$\\_$r$\\%$.txt**, and the corresponding labels are provided with the file names **y$\\_$train$\\_$r$\\%$.txt** in the same folder). In each case, measure both the $L_1$ and $L_2$ error on the training examples used, as well as the error on the given test set. Plot a curve showing both errors (on the *y-axis* as a function of the number of training examples used (on the *x-axis*).\n",
        "\n",
        "Add the resulting curve to your Latex document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gGq3BStoB12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Please remember to comment out all code not contained within a function before \n",
        "submitting your file.\n",
        "\"\"\"\n",
        "\n",
        "X_test = pd.read_csv('/content/hw2_data/hw2_data/Data-set-1/X_test.txt', header=None).values\n",
        "y_test = pd.read_csv('/content/hw2_data/hw2_data/Data-set-1/y_test.txt', header=None).values\n",
        "\n",
        "#TODO your code here: Question 1.1.1.\n",
        "L1_train_errors = [0] * 10\n",
        "L2_train_errors = [0] * 10\n",
        "L1_test_errors = [0] * 10\n",
        "L2_test_errors = [0] * 10\n",
        "\n",
        "\n",
        "plt.plot(range(10),L1_train_errors, label ='L1_train')\n",
        "plt.plot(range(10),L1_test_errors, label ='L1_test')\n",
        "plt.plot(range(10),L2_train_errors, label = 'L2_train')\n",
        "plt.plot(range(10),L2_test_errors, label = 'L2_test')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.xlabel(\"Percent of Training Data\")\n",
        "plt.xticks(range(10), range(10,101,10))\n",
        "plt.ylabel(\"Error\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giK5b9ukoB1_",
        "colab_type": "text"
      },
      "source": [
        "### Q1.1.2: Analysis of model learned from full training data\n",
        "\n",
        "Write down the weight and bias terms, $\\hat{w}$ and $\\hat{b}$, learned from the full training data in your Latex document. Also, write down the $L_2$ training and test error of this model. In a single figure, draw a plot of the learned linear function (input instance on the *x-axis* and the predicted value on the *y-axis*), along with a scatter plot depicting the true label associated with each test instance.\n",
        "\n",
        "Add the resulting plot to your Latex document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvVwctBdoB2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Please remember to comment out all code not contained within a function before \n",
        "submitting your file.\n",
        "\"\"\"\n",
        "\n",
        "X_test = pd.read_csv('/content/hw2_data/hw2_data/Data-set-1/X_test.txt', header=None).values\n",
        "y_test = pd.read_csv('/content/hw2_data/hw2_data/Data-set-1/y_test.txt', header=None).values\n",
        "X_train = pd.read_csv('/content/hw2_data/hw2_data/Data-set-1/X_train.txt',header=None).values\n",
        "y_train = pd.read_csv('/content/hw2_data/hw2_data/Data-set-1/y_train.txt', header=None).values\n",
        "\n",
        "#TODO your code here: Question 1.1.2.\n",
        "\n",
        "\n",
        "\n",
        "x = np.linspace(0,1)\n",
        "plt.plot(x,(x * w + b).T, color='red')\n",
        "plt.scatter(X_test, y_test)\n",
        "plt.title('Model from Full Training Data')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P20rbONVoB2P",
        "colab_type": "text"
      },
      "source": [
        "### Q1.2.1-2: Regression on different portions of training data\n",
        "\n",
        "#### Regression on 5$\\%$ of the training data. \n",
        "Use your implementation of $L_2$-regularized least squares regression to learn a model on 5$\\%$ of the training data. Select the regularization parameter from the range $\\{$0.1,1,10,50,100,150,200,500,1000,2500,5000$\\}$ using 5-fold cross validation on the relevant training data. Draw a plot showing $\\lambda$ on the *x-axis* and the training, test, and cross validation errors on the *y-axis* using the $L_2$ error. \n",
        "\n",
        "Then record the chosen value of $\\lambda$ along with the weight vector, bias term, and all corresponding errors for the chosen value of $\\lambda$.\n",
        "        \n",
        "#### Regression on 100$\\%$ of the training data.\n",
        "\n",
        "Repeat the above process, but instead learn from the full training data for $L_2$-regularized regression. Plot all of the errors, and record the chosen value of $\\lambda$ along with the weight vector, bias term, and all corresponding errors for the chosen value of $\\lambda$.\n",
        "\n",
        "Add the resulting curves to your Latex document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssEm895ToB2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Please remember to comment out all code not contained within a function before \n",
        "submitting your file.\n",
        "\"\"\"\n",
        "\n",
        "for i in [5, 100]:\n",
        "    lambda_ = [0.1,1,10,50,100,150,200,500,1000,2500,5000]\n",
        "    train_error = [0]*11\n",
        "    test_error = [0]*11\n",
        "    cv_error = [0]*11\n",
        "    X_test = pd.read_csv('/content/hw2_data/hw2_data/Data-set-2/X_test',header=None).values\n",
        "    y_test = pd.read_csv('/content/hw2_data/hw2_data/Data-set-2/y_test',header=None).values\n",
        "    X_train = pd.read_csv('/content/hw2_data/hw2_data/Data-set-2/Train-subsets/X_train_'+str(i)+'.txt',header=None).values\n",
        "\n",
        "    y_train = pd.read_csv('/content/hw2_data/hw2_data/Data-set-2/Train-subsets/y_train_'+str(i)+'.txt',header=None).values \n",
        "    \n",
        "    \n",
        "    #TODO your code here: Question 1.2.1-2\n",
        "    \n",
        "    \n",
        "    \n",
        "    plt.plot(range(11), train_error, label='training error')\n",
        "    plt.plot(range(11),cv_error, label = 'cv error')\n",
        "    plt.plot(range(11), test_error, label = 'test error')\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "    plt.title('Regression on ' + str(i) + '% of the training data')\n",
        "    plt.ylabel('L2 error')\n",
        "    plt.xlabel('Lambda Value')\n",
        "    plt.xticks(range(11), lambdas)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zkrzJ8LYWNv",
        "colab_type": "text"
      },
      "source": [
        "### Q1.2 Report on Latex\n",
        "Answer the following questions on Latex in the respective section.\n",
        "\n",
        "1.2.3 For each of the two training sets considered above (5$\\%$ and 100$\\%$), compare the training and test errors of the models learned using unregularized least squares regression and ridge regression. What can you conclude from this about the value of regularization for small and large training sets?\n",
        "\n",
        "1.2.4 For each of the two training sets considered above (5$\\%$ and 100$\\%$), Which $\\lambda$ should be larger by theory?why? Do those values align with the conclusion you made in part 1.3?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOST5QSSXcCR",
        "colab_type": "text"
      },
      "source": [
        "## Q2: Batch Gradient Descent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FG2ga9IoB1q",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Descent Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iOD4J1noB1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LinearRegressionGD(train_data, train_labels, iters, learning_rate):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        train_data ((n,p) np.array): n is the number of training points and p the number of features\n",
        "        train_labels ((n,1) np.array): training labels for the training data \n",
        "        iters (int): the number of iterations to run Gradient Descent\n",
        "        learning_rate (float): the alpha value for gradient descent\n",
        "\n",
        "    Returns\n",
        "        tuple: (w, b) where w is a (p,1) weight vector, and b the bias term \n",
        "    \"\"\"\n",
        "    w = np.array()\n",
        "    b = 0\n",
        "    \n",
        "    return (w, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxQ_BL9IoB2q",
        "colab_type": "text"
      },
      "source": [
        "### Q2.1-2.2: OLS and Gradient Descent Runtime\n",
        "\n",
        "2.1) Time  the  closed-form  unregularized  linear  regression  implementation  you  wrote  inprevious section on the full training data for Data Set 1.  Write down the weight and bias terms,  ˆwandˆb, learned from the full training data, as well as theL2error on the test data, and the time it took torun the full process.\n",
        "\n",
        "2.2) Time the gradient descent implementation you just wrote on the full training data for Data Set 1 with iterations from range $\\{$10, 100, 1000$\\}$, and a learning rate of 0.01. Write down the weight and bias terms , $\\hat{w}$ and $\\hat{b}$, learned from the full training data in your Latex document, as well as the $L_2$ error on the test data, and the time it took to run the full process. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtVVF2hGoB2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Please remember to comment out all code not contained within a function before \n",
        "submitting your file.\n",
        "\"\"\"\n",
        "\n",
        "X_test = pd.read_csv('/content/hw2_data/hw2_data/Data-set-1/X_test.txt', header=None).values\n",
        "y_test = pd.read_csv('/content/hw2_data/hw2_data/Data-set-1/y_test.txt', header=None).values\n",
        "X_train = pd.read_csv('/content/hw2_data/hw2_data/Data-set-1/X_train.txt', header=None).values\n",
        "y_train = pd.read_csv('/content/hw2_data/hw2_data/Data-set-1/y_train.txt', header=None).values\n",
        "\n",
        "start = time.time()\n",
        "(w,b) = LinearRegression(X_train, y_train)\n",
        "y_test_pred = np.matmul(X_test, w) + b\n",
        "y_train_pred = np.matmul(X_train, w) + b\n",
        "\n",
        "\n",
        "L2_error(y_test, y_test_pred)\n",
        "print(\"--- OLS Closed Form ---\")\n",
        "print(\"W:\", w.T, \"B:\", b)\n",
        "print(\"L2 Training Error:\", L2_error(y_train, y_train_pred))\n",
        "print(\"L2 Test Error:\", L2_error(y_test, y_test_pred))\n",
        "print(\"Time:\", time.time() - start)\n",
        "print()\n",
        "\n",
        "start = time.time()\n",
        "(w,b) = LinearRegressionGD(X_train, y_train, 10, .01)\n",
        "y_test_pred = np.matmul(X_test, [w]) + b\n",
        "y_train_pred = np.matmul(X_train, [w]) + b\n",
        "L2_error(y_test, y_test_pred)\n",
        "print(\"---  Gradient Descent ---\")\n",
        "print(\"W:\", w, \"B:\", b)\n",
        "print(\"L2 Training Error:\", L2_error(y_train, y_train_pred))\n",
        "print(\"L2 Test Error:\", L2_error(y_test, y_test_pred))\n",
        "print(\"Time:\", time.time() - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e57oBdHBa4i0",
        "colab_type": "text"
      },
      "source": [
        "### Q2.3 Report on Latex\n",
        "Answer the followings question on Latex in the respective section.\n",
        "\n",
        "Which algorithm runs faster? Why might that be the case? Why would we ever use gradient descent linear regression in practice while a closed form solution exists?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqECw4f6LJCx",
        "colab_type": "text"
      },
      "source": [
        "## Turning it in\n",
        "\n",
        "\n",
        "**Remember to recomment all script portions of this notebook before submitting (i.e. any code not in a function, excluding code that imports libraries). This is to ensure that the Autograder works properly. Also make sure you did not edit other sections of the code outside of specified areas.**\n",
        "\n",
        "1. Download this notebook as a `hw2.ipynb` file with the functions implemented and the sandbox code commented out\n",
        "  - If using Google Colab, go to \"File -> Download .ipynb\"\n",
        "  - If using Jupyter locally, go to \"File -> Download as -> Notebook (.ipynb)\"\n",
        "  \n",
        "2. Submit `hw2.ipynb` file to Gradescope (you can do this as many times as you'd like before the deadline)"
      ]
    }
  ]
}